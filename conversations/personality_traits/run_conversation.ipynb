{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4265ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nobel\\AppData\\Local\\Temp\\ipykernel_28584\\3682597166.py:20: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(value=[(\"assistant\", initial_message)], label=\"Soul Match\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph initial invocation result: {'messages': [AIMessage(content='üéâ Congratulations on being with us so far! It\\'s great to have you here. I\\'m excited to explore more about your personality and get to know you better. Are you ready to dive in and discover more about yourself? If so, just say \"yes\" or \"no\"! üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 83, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BppxYdODd9kgUyHMKyUYIXB7Q0thu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--87a1f2a6-7fb7-4d9f-b853-207005b19661-0', usage_metadata={'input_tokens': 83, 'output_tokens': 57, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'personality_traits_covered': [], 'current_personality_trait': 'Imagination', 'is_message_approved': True, 'message_feedback': '', 'current_user_message': '', 'is_facetwise_conversation_finished': False, 'facetwise_conversations': {'Imagination': [], 'Artistic Interests': []}, 'is_all_objective_completed': False}\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph invocation result: {'messages': [AIMessage(content='üéâ Congratulations on being with us so far! It\\'s great to have you here. I\\'m excited to explore more about your personality and get to know you better. Are you ready to dive in and discover more about yourself? If so, just say \"yes\" or \"no\"! üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 83, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BppxYdODd9kgUyHMKyUYIXB7Q0thu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--87a1f2a6-7fb7-4d9f-b853-207005b19661-0', usage_metadata={'input_tokens': 83, 'output_tokens': 57, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='yes', additional_kwargs={}, response_metadata={}, id='786846c5-2677-4402-89f0-3fe4fdc9aca9'), AIMessage(content=\"Hey there! I'm curious, do you have a favorite book or movie that you feel really sparks your imagination? \", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 337, 'total_tokens': 360, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BppxetIItT0BJjFRyweTXAZ7zcFye', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--02dcf592-7908-45e4-9af7-7dd2d04b7295-0', usage_metadata={'input_tokens': 337, 'output_tokens': 23, 'total_tokens': 360, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'personality_traits_covered': [], 'current_personality_trait': 'Imagination', 'is_message_approved': True, 'message_feedback': 'The response made by the assistant was friendly and open, encouraging the user to share their interests. By asking about a favorite book or movie that sparks imagination, it adheres to the guideline of being curious and naturally engaging. There is only one simple question, and it fosters a flowing conversation rather than feeling like a survey. Overall, it aligns well with the behavioral guidelines, making it an effective response.', 'current_user_message': '', 'is_facetwise_conversation_finished': False, 'facetwise_conversations': {'Imagination': [SystemMessage(content=\"\\n      You are a friendly, curious, emotionally aware chatbot. \\n      You're having a relaxed, one-on-one conversation with a user.\\n      Your goal is to understand how strong the Imagination facet of\\n      Openness to Experience is in the user‚Äôs personality.\\n\\n      \\n\\n\\n      Instructions:\\n      1. Keep the conversation casual and engaging.\\n      2. Keep the tone warm, curious, non-judgmental, and emotionally intelligent.\\n      3. Ask about the user's personal life or past experiences in a way that gently\\n        reveals their inner imaginative world.\\n      4. Usually books , movies, art, or creative hobbies are good topics to explore.\\n      5. Use the previous conversation as context to ask maximum one follow up question if needed\\n        to show more curisity.\\n      6. If the user's message shows no such trait, ask a new question to open up another\\n        part of their inner world.\\n      7. Avoid: Abstract psychological terms like ‚Äúimagination,‚Äù ‚Äútrait,‚Äù or ‚Äúopenness.‚Äù \\n        Focus on real-life examples.\\n      8. If there is no previous conversation, start with a new question to open up\\n        the conversation.\\n\\n      \\n\\n      Questioning Style:\\n      1. Ask only one question at a time.\\n      2. No compound questions. Stick to a single topic per message.\\n      3. Avoid sounding like a survey. Let the user feel you're naturally\\n        interested in their story.\\n      \\n\\n\\n      Use the conversation history to avoid repeating \\n      themes and to build on what the user has already said.\\n    \", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey there! I'm curious, do you have a favorite book or movie that you feel really sparks your imagination? \", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 337, 'total_tokens': 360, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BppxetIItT0BJjFRyweTXAZ7zcFye', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--02dcf592-7908-45e4-9af7-7dd2d04b7295-0', usage_metadata={'input_tokens': 337, 'output_tokens': 23, 'total_tokens': 360, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'Artistic Interests': []}, 'is_all_objective_completed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\MyLLmProjects\\soul_match\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\MyLLmProjects\\soul_match\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\MyLLmProjects\\soul_match\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\MyLLmProjects\\soul_match\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\MyLLmProjects\\soul_match\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\MyLLmProjects\\soul_match\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\MyLLmProjects\\soul_match\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\MyLLmProjects\\soul_match\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 904, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\nobel\\AppData\\Local\\Temp\\ipykernel_28584\\3682597166.py\", line 24, in user_chat\n",
      "    return chat(user_message, chat_history)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\nobel\\AppData\\Local\\Temp\\ipykernel_28584\\3682597166.py\", line 15, in chat\n",
      "    response = graph.run_graph(message)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\MyLLmProjects\\soul_match\\conversations\\personality_traits\\graph.py\", line 102, in run_graph\n",
      "    result = self.graph.invoke(state, config=config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\MyLLmProjects\\soul_match\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 2719, in invoke\n",
      "    for chunk in self.stream(\n",
      "                 ^^^^^^^^^^^^\n",
      "  File \"c:\\MyLLmProjects\\soul_match\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 2456, in stream\n",
      "    raise GraphRecursionError(msg)\n",
      "langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from graph import PersonalityTraitsGraph\n",
    "\n",
    "\n",
    "graph = PersonalityTraitsGraph()\n",
    "\n",
    "graph.setup()\n",
    "\n",
    "initial_message = graph.run_graph_initially()\n",
    "\n",
    "def chat(message, history):\n",
    "    if not history:  # If this is the first user message, add the initial assistant message\n",
    "        history = [(\"assistant\", initial_message)]\n",
    "    history.append((\"user\", message))\n",
    "    response = graph.run_graph(message)\n",
    "    history.append((\"assistant\", response))\n",
    "    return history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(value=[(\"assistant\", initial_message)], label=\"Soul Match\")\n",
    "    msg = gr.Textbox()\n",
    "\n",
    "    def user_chat(user_message, chat_history):\n",
    "        return chat(user_message, chat_history)\n",
    "\n",
    "    msg.submit(user_chat, [msg, chatbot], chatbot)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66fe6aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soul-match",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
